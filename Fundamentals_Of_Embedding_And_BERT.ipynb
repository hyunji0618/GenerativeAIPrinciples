{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Fundamentals of Embedding and BERT\n",
    "\n",
    "**University of Chicago**\n",
    "**MS in Applied Data Science**\n",
    "\n",
    "Course: Generative AI Principles \n",
    "\n",
    "Date: 04/05/2025\n",
    "\n",
    "Author: Hyunji Amy Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>Safe Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>Phishing Email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Email Text  \\\n",
       "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
       "1           1  the other side of * galicismos * * galicismo *...   \n",
       "2           2  re : equistar deal tickets are you still avail...   \n",
       "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
       "4           4  software at incredibly low prices ( 86 % lower...   \n",
       "\n",
       "       Email Type  \n",
       "0      Safe Email  \n",
       "1      Safe Email  \n",
       "2      Safe Email  \n",
       "3  Phishing Email  \n",
       "4  Phishing Email  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Phishing_Email.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Email Classification using KNN (With first 1000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 1000 rows\n",
    "df_subset = df.iloc[:1000].copy()\n",
    "\n",
    "# Drop rows with missing 'Email Text'\n",
    "df_subset.dropna(subset=['Email Text'], inplace=True)\n",
    "\n",
    "# Vectorize the email text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df_subset['Email Text'])\n",
    "\n",
    "# Encode the labels ('Phishing Email' and 'Safe Email' to numeric labels)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_subset['Email Type'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "f1 = f1_score(y_test, knn.predict(X_test), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.57\n",
      "F1 Score:  0.5240067911714771\n",
      "Classification Report: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Phishing Email       0.49      0.99      0.65        82\n",
      "    Safe Email       0.97      0.28      0.43       118\n",
      "\n",
      "      accuracy                           0.57       200\n",
      "     macro avg       0.73      0.63      0.54       200\n",
      "  weighted avg       0.77      0.57      0.52       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Classification Report: \\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:** \n",
    "\n",
    "Using K-Nearest Neighbors (KNN) without BERT resulted in an overall accuracy of 57% and an F1 score of 0.52, suggesting that the model is only slightly better than random guessing and struggles to balance false positives and false negatives. \n",
    "\n",
    "While it achieved high recall (0.99) for phishing emails, it performed poorly in identifying safe emails (recall = 0.28), indicating a strong bias toward predicting emails as phishing. \n",
    "\n",
    "As a result, the KNN model produced a high number of false positives for phishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparing [CLS] Token and Average Token Embeddings\n",
    "\n",
    "BERT provides embeddings for each token of the input text, including: \n",
    "\n",
    "**[CLS] token embedding:** Represent the entire sequence for classification tasks. \n",
    "\n",
    "**Average token embedding:** Average the embeddings of all tokens. \n",
    "\n",
    "This section explores the differences between using the [CLS] token embedding and the average of all token embeddings for classification.\n",
    "\n",
    "#### (1) Calculating the average embedding for all tokens in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text and labels\n",
    "texts = df_subset[\"Email Text\"].tolist()\n",
    "labels = df_subset[\"Email Type\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Phishing Email' and 'Safe Email' to numeric labels (e.g., 0 and 1)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model (base uncased version)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()  # Set model to inference mode (no training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings ([CLS] and Average)\n",
    "def get_bert_embeddings(texts):\n",
    "    cls_embeddings = []\n",
    "    avg_embeddings = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for text in texts:\n",
    "            # Tokenize and encode the text (This line automatically marks the text before Tokenizing)\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Last hidden states of all tokens (shape: 1, seq_len, hidden_dim)\n",
    "            last_hidden_state = outputs.last_hidden_state.squeeze(0)  # Remove batch dim\n",
    "\n",
    "            # [CLS] token embedding is always the first token (index 0)\n",
    "            cls_emb = last_hidden_state[0].numpy()\n",
    "\n",
    "            # Average of all token embeddings\n",
    "            avg_emb = last_hidden_state.mean(dim=0).numpy()\n",
    "\n",
    "            cls_embeddings.append(cls_emb)\n",
    "            avg_embeddings.append(avg_emb)\n",
    "\n",
    "    return np.array(cls_embeddings), np.array(avg_embeddings)\n",
    "\n",
    "# Get embeddings\n",
    "cls_embs, avg_embs = get_bert_embeddings(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((998, 768), (998, 768))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the shape\n",
    "cls_embs.shape, avg_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Comparing the performance using F1 scores of a KNN classifier using these two embedding (CLS embedding and average embedding) methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80% training, 20% testing) for KNN\n",
    "X_train_cls, X_test_cls, y_train, y_test = train_test_split(cls_embs, labels_encoded, test_size=0.2, random_state=42)\n",
    "X_train_avg, X_test_avg, _, _ = train_test_split(avg_embs, labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with [CLS] embedding \n",
    "knn_cls = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_cls.fit(X_train_cls, y_train)\n",
    "y_pred_cls = knn_cls.predict(X_test_cls)\n",
    "\n",
    "# KNN with average embedding \n",
    "knn_avg = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_avg.fit(X_train_avg, y_train)\n",
    "y_pred_avg = knn_avg.predict(X_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score using [CLS] embedding: 0.9301225414478427\n",
      "F1 Score using average embedding: 0.9350586718246293\n"
     ]
    }
   ],
   "source": [
    "# F1 Scores\n",
    "f1_cls = f1_score(y_test, y_pred_cls, average=\"weighted\")\n",
    "f1_avg = f1_score(y_test, y_pred_avg, average=\"weighted\")\n",
    "\n",
    "print(\"F1 Score using [CLS] embedding:\", f1_cls)\n",
    "print(\"F1 Score using average embedding:\", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report using [CLS] embedding: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Phishing Email       0.90      0.93      0.92        82\n",
      "    Safe Email       0.95      0.93      0.94       118\n",
      "\n",
      "      accuracy                           0.93       200\n",
      "     macro avg       0.93      0.93      0.93       200\n",
      "  weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "Classification Report using average embedding: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Phishing Email       0.92      0.93      0.92        82\n",
      "    Safe Email       0.95      0.94      0.94       118\n",
      "\n",
      "      accuracy                           0.94       200\n",
      "     macro avg       0.93      0.93      0.93       200\n",
      "  weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Reports\n",
    "report_cls = classification_report(y_test, y_pred_cls, target_names=label_encoder.classes_)\n",
    "report_avg = classification_report(y_test, y_pred_avg, target_names=label_encoder.classes_)\n",
    "\n",
    "print(\"Classification Report using [CLS] embedding: \\n\", report_cls)\n",
    "print(\"Classification Report using average embedding: \\n\", report_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "Using BERT embeddings significantly improved the KNN model’s performance. \n",
    "\n",
    "With the [CLS] embedding, the model achieved an accuracy of 93% and an F1 score of 0.93. \n",
    "\n",
    "When using the average embedding, the performance improved slightly, reaching an accuracy of 94% and an F1 score of 0.94. \n",
    "\n",
    "Both methods achieved high precision and recall across classes, but the average embedding showed a slight edge, particularly in capturing more safe emails correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare 768 dimensional embedding vs 2 dimensional embedding knn results using UMAP and PCA dimensional reduction techniques\n",
    "\n",
    "This section explores how dimensionality reduction techniques, specifically UMAP and PCA, affect the performance of a KNN classifier when using high-dimensional BERT embeddings by comparing the classifier's performance using original 768-dimensional embeddings against 2-dimensional embeddings obtained through dimensionality reduction.\n",
    "\n",
    "#### (1) Apply PCA and UMAP to reduce BERT embeddings ([CLS]) to 2 dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from umap-learn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from umap-learn) (1.6.1)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.61.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from umap-learn) (4.67.1)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading numba-0.61.0-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading llvmlite-0.44.0-cp313-cp313-macosx_11_0_arm64.whl (26.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.44.0 numba-0.61.0 pynndescent-0.5.13 umap-learn-0.5.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca_model = PCA(n_components=2) # PCA\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42) # UMAP\n",
    "\n",
    "cls_pca = pca_model.fit_transform(cls_embs)\n",
    "cls_umap = umap_model.fit_transform(cls_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((998, 2), (998, 2))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if dimension is correctly reduced\n",
    "cls_pca.shape, cls_umap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Train a KNN classifier using the original embeddings ([CLS]), PCA-reduced embeddings, and UMAP-reduced embeddings ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split on reduced\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(cls_pca, labels, test_size=0.2, random_state=42)\n",
    "X_train_umap, X_test_umap, _, _ = train_test_split(cls_umap, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN on reduced embeddings\n",
    "knn.fit(X_train_pca, y_train)\n",
    "knn.fit(X_train_umap, y_train)\n",
    "\n",
    "y_pred_pca = knn.predict(X_test_pca)\n",
    "y_pred_umap = knn.predict(X_test_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Compare the classifiers' performances using F1 scores to understand the impact of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - CLS: 0.9301225414478427\n",
      "F1 Score - AVG: 0.9350586718246293\n",
      "F1 Score - PCA (CLS→2D): 0.2598095238095238\n",
      "F1 Score - UMAP (CLS→2D): 0.9196666666666666\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "f1_pca = f1_score(y_test, y_pred_pca, average=\"weighted\")\n",
    "f1_umap = f1_score(y_test, y_pred_umap, average=\"weighted\")\n",
    "\n",
    "print(\"F1 Score - CLS:\", f1_cls)\n",
    "print(\"F1 Score - AVG:\", f1_avg)\n",
    "print(\"F1 Score - PCA (CLS→2D):\", f1_pca)\n",
    "print(\"F1 Score - UMAP (CLS→2D):\", f1_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report using [CLS] embedding: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Phishing Email       0.41      1.00      0.59        82\n",
      "    Safe Email       1.00      0.02      0.03       118\n",
      "\n",
      "      accuracy                           0.42       200\n",
      "     macro avg       0.71      0.51      0.31       200\n",
      "  weighted avg       0.76      0.42      0.26       200\n",
      "\n",
      "Classification Report using average embedding: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Phishing Email       0.92      0.88      0.90        82\n",
      "    Safe Email       0.92      0.95      0.93       118\n",
      "\n",
      "      accuracy                           0.92       200\n",
      "     macro avg       0.92      0.91      0.92       200\n",
      "  weighted avg       0.92      0.92      0.92       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Reports\n",
    "report_pca = classification_report(y_test, y_pred_pca, target_names=label_encoder.classes_)\n",
    "report_umap = classification_report(y_test, y_pred_umap, target_names=label_encoder.classes_)\n",
    "\n",
    "print(\"Classification Report using [CLS] embedding: \\n\", report_pca)\n",
    "print(\"Classification Report using average embedding: \\n\", report_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "Using PCA, the model's performance dropped drastically with only 42% accuracy and an F1 score of 0.26. In addition, the classifier predicts nearly all emails as phishing. \n",
    "\n",
    "UMAP preserved most of the original embedding's structure, achieving an accuracy of 92% and an F1 score of 0.92, nearly matching the full-dimensional average embedding performance. \n",
    "\n",
    "This showed that UMAP is far more effective than PCA for preserving semantic information in low-dimensional space when using KNN with BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
